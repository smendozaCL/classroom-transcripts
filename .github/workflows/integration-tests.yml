name: Integration Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  integration-tests:
    runs-on: ubuntu-latest

    services:
      azurite:
        image: mcr.microsoft.com/azure-storage/azurite
        ports:
          - 10000:10000
          - 10001:10001
          - 10002:10002

    env:
      AZURE_STORAGE_ACCOUNT: devstoreaccount1
      AZURE_STORAGE_CONNECTION_STRING: DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;
      ASSEMBLYAI_API_KEY: ${{ secrets.ASSEMBLYAI_API_KEY }}
      AZURE_FUNCTION_KEY: ${{ secrets.AZURE_FUNCTION_KEY }}
      WEBSITE_HOSTNAME: ${{ secrets.WEBSITE_HOSTNAME }}
      PYTHONUNBUFFERED: 1

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r src/functions/requirements.txt
          pip install pytest pytest-azurepipelines pytest-cov pytest-xdist

      - name: Download test audio file
        run: |
          mkdir -p data
          curl -L -o data/short-classroom-sample.m4a https://github.com/eddo-ai/classroom-transcripts/raw/main/data/short-classroom-sample.m4a || \
          curl -L -o data/short-classroom-sample.m4a https://github.com/eddo-ai/classroom-transcripts/raw/develop/data/short-classroom-sample.m4a || \
          echo "Creating dummy audio file for testing" > data/short-classroom-sample.m4a

      - name: Create test directories
        run: |
          mkdir -p logs
          mkdir -p test-results
          mkdir -p test-results/coverage-html

      - name: Run integration tests
        run: |
          pytest tests/ -v -m "integration" \
            --junitxml=test-results/junit.xml \
            --cov=src \
            --cov-report=xml:test-results/coverage.xml \
            --cov-report=html:test-results/coverage-html \
            -n auto

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results
          path: |
            test-results/
            logs/

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: success()
        with:
          file: test-results/coverage.xml
          flags: integration-tests
          name: codecov-integration
          fail_ci_if_error: true

      - name: Process test results
        if: always()
        run: |
          echo "::group::Test Summary"
          python -c "
          import os
          import xml.etree.ElementTree as ET

          junit_path = 'test-results/junit.xml'
          if not os.path.exists(junit_path):
              print('No test results file found')
              exit(0)

          try:
              tree = ET.parse(junit_path)
              root = tree.getroot()
              
              # Different pytest XML formats might use different attribute names
              tests = int(root.attrib.get('tests', 0))
              failures = int(root.attrib.get('failures', 0))
              errors = int(root.attrib.get('errors', 0))
              skipped = int(root.attrib.get('skipped', 0))
              
              print(f'Total Tests: {tests}')
              print(f'Passed: {tests - failures - errors - skipped}')
              print(f'Failed: {failures}')
              print(f'Errors: {errors}')
              print(f'Skipped: {skipped}')
              
              # Print test case details
              print('\nTest Case Details:')
              for testcase in root.findall('.//testcase'):
                  name = testcase.attrib.get('name', 'Unknown')
                  result = 'Passed'
                  
                  failure = testcase.find('failure')
                  error = testcase.find('error')
                  skipped = testcase.find('skipped')
                  
                  if failure is not None:
                      result = f'Failed: {failure.text}'
                  elif error is not None:
                      result = f'Error: {error.text}'
                  elif skipped is not None:
                      result = 'Skipped'
                  
                  print(f'- {name}: {result}')

          except Exception as e:
              print(f'Error processing test results: {str(e)}')
              exit(0)
          "
          echo "::endgroup::"

          if [ -d "logs" ]; then
            echo "::group::Test Logs"
            for log in logs/*.log; do
              if [ -f "$log" ]; then
                echo "=== $log ==="
                tail -n 50 "$log"
                echo
              fi
            done
            echo "::endgroup::"
          fi
